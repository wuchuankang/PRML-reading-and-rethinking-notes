# 最大似然函数、交叉熵和经验分布的关系

## 经验分布

* 最大似然函数能推导出交叉熵，他们俩本质是一回事，推导的桥梁就是经验分布。经验分布的定义：
  $$
  \begin{equation}
  \widehat p_{data}(x)=\left\{
  \begin{array}{rcl}
  0 & & {x < x_1}\\
  k/n & & {x_k \leq x < x_{k+1}}\\
  1 & & {x \geq x_{k+1}}
  \end{array} \right.\tag 1
  \end{equation}
  $$
  这是对连续函数而言的，其中$x_k​$是已经重新按照大小排列过得。对于离散函数，则 $\widehat p_{data}=k/n​$ .

- 考虑到分类问题（离散型），最大似然估计：
  $$
  ![](http://....) P(X;\theta)=\frac1 N\prod_kq^{N*p_k}(C_k;\theta) \tag2
  $$
  其中$p_k​$是第k类出现的频率，也就是经验分布，对似然估计取$log​$似然，得到：
  $$
  L=\sum_k(p_k)logq(C_k,\theta) \tag {3}
  $$
  显然$(3)​$式就是交叉熵$H(p,q) = -\int{p(x)}\log{q(x)} dx=-\sum_k{p_k}{\log q_k}​$

* 考虑回归问题（连续型） ，给定一组含有$m​$个样本的$X=\{x^{(1)},...x^{(m)}\}​$，独立的由真是数据生成分布$P_{data}​$生成，$P_{model}​$是一族由$\theta​$为参数的概率分布，用来估计真实的概率分布$P_{data}​$，最大似然估计：
  $$
  \begin{equation}
  \begin{split}
  \theta_{ML}
  &=\arg\max\limits_{\theta}\prod_i^N{p_{model}(x_i;\theta)}\\
  &=\arg\max\limits_{\theta}\sum_i^N{\log p_{model}(x_i;\theta)}
  \end{split} \tag 4
  \end{equation}
  $$
  对上式乘以$\frac {1}{N}​$不影响结果，则可以用经验分布$\widehat p_{data}​$相关的经验期望作为准则：
  $$
  \begin{equation}
  \begin{split}
  \theta_{ML} 
  &= \arg\max\limits_{\theta}E_{x\sim{\widehat p_{data}}}p(\bold x;\theta)\\
  &= \arg\max\limits_{\theta}\sum_i^N\{\widehat p_{data}(x_i)\log p_{model}(x_i;\theta)\}\\
  &=\arg\max\limits_{\theta}-H(\widehat p_{data},p_{model})\\
  &=\arg\min\limits_{\theta}H(\widehat p_{data}, p_{model})
  \end{split} \tag 5
  \end{equation}
  $$
  可以看出最大似然函数等价于最小交叉熵。

  ## 总结

  任何由负似然函数组成的损失都是定义在训练集上的经验函数和定义在模型上的概率分布之间的交叉熵。比如均方损失就是经验分布和模型是高斯分布之间的交叉熵。

  
