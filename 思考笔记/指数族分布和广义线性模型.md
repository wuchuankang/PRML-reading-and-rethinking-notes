##  指数族分布和广义线性模型

* 广义线性模型自然包括普通的线性模型，普通线性模型是：
  $$
  y=\bold w^T\bold x  \ \ \ \  or \ \ \  \  y=\bold W^T\bold x  \tag1
  $$
  广义线性模型一般是间接构造的，现在把构造的逻辑讲一下：

  构造的模型用$h_{\bold w}(\bold x)​$ 表示，即给定一个输入$\bold x​$，$h_{\bold w}(\bold x)​$给出预测。我们知道$h_{\bold w}(\bold x)​$应该是：
  $$
  h_{\bold w}(\bold x)=E( y|\bold x;\bold w)   \tag 2
  $$
  那么如果我们知道概率分布$p( y|\bold x;\eta(\bold w))$，当然这个概率分布有未知参数，用$\eta(\bold w)$表示，为什么$\eta$ 是$\bold w$的函数？其实不难理解，因为$E (y|\bold x;\bold w)$就是概率分布$p(y|\bold x;\eta)$的期望，显然$\eta$ 和$\bold w$是有函数关系的，可以用$\eta(\bold w)$来表示。当$\eta$ 和$\bold w$是线性关系时，即：$\eta(\bold w)=\bold w^T \bold x$，或者$\eta(\bold w)=\bold w^T \phi(\bold x)$，就是广义线性模型。

  那么问题就变为找$E( y|\bold x;\bold w)​$和$\eta​$的关系。

  就当前接触到的概率模型：bernulli分布，二项分布，beta分布，多项式分布，$Dirichlet​$分布，高斯分布，泊松分布都是指数分布族中的分布。

  为了书写方便，将$p( y|\bold x;\eta)$的$\bold x$省略，写成$p( y|\eta)​$。

  所以假设概率分布$p( y|\eta)​$ 是指数族分布，其定义：
  $$
  p(y|\eta)=b(y)\exp(\eta^TT(y)-a(\eta)) \ \ \  \ or \ \ \ \ p(y|\eta)=\frac{b(y)}{a(\eta)}\exp(\eta^TT(y))\tag 3
  $$
  其中：$\eta$ 是自然参数，$T(y)$是充分统计量，一般$T(y)=y$，$a(\eta)$是用于保证概率正交化的量。

  1. bernoulli 分布：二分问题
     $$
     p(y|\mu)=Bern(y|\mu)=\mu^{y}(1-\mu)^{1-y}  \tag 4
     $$
     表达成指数形式：
     $$
     \begin{aligned}
     p(y|\mu)
     &=\exp(y\ln\mu+(1-y)\ln(1-\mu))\\
     &=(1-\mu)\exp(\ln(\frac{\mu}{1-\mu})y)
     \end{aligned}   \tag{4.1}
     $$
     可以看出：
     $$
     \eta=\ln{\frac{\mu}{1-\mu}}   \tag{4.2}
     $$
     Bern分布的期望：
     $$
     E(y|\eta)=1×\mu+0×(1-\mu)=\mu  \tag{4.3}
     $$
     由$(4.2)$可以得出：
     $$
     \mu=\frac{1}{1+\exp(-\eta)}=\sigma(\eta) \tag{4.4}
     $$
     注意$\sigma(\eta)$就是$sigmoid$函数。

     所以，我们建立的线性模型就是：
     $$
     h_{\bold w}(\bold x)=E( y|\bold x;\bold w)=\mu=\sigma(\eta(\bold w))=\frac{1}{1+\exp(-\bold w^T\bold x)}  \tag {4.5}
     $$
     这就是$logistical$模型。

  2. 多项式分布：多分类问题
     $$
     p(\bold y|\boldsymbol \mu)=\prod_{k=1}^{K}\mu_k^{y_k}=\exp \sum_{k=1}^K y_k\ln \mu_k=\exp(\boldsymbol \eta^T\bold y)  \tag{5}
     $$
     其中：$\boldsymbol \eta^T=[\ln\mu_1,...,\ln\mu_K] , \ \ \ \ \ \ \sum_{k=1}^K \mu_k =1​$

     可以看出：
     $$
     \eta_k=\ln\mu_k \Longrightarrow \mu_k=\exp(\eta_k)  \tag{5.1}
     $$

     $$
     \sum_{k=1}^K \mu_k =1 \Longrightarrow \sum_{k=1}^K \exp(\eta_k)=1  \tag{5.2}
     $$

     从而可以得到：
     $$
     \mu_k=\frac {\exp(\eta_k)}{1}=\frac {\exp(\eta_k)}{\sum_{l=1}^K \exp(\eta_l)} \tag{5.3}
     $$
     这就是$softmax$函数。

     多项式分布的期望是：
     $$
     E[\bold y|\boldsymbol \mu]=\sum_{\bold x}p(\bold y|\boldsymbol \mu)=(\mu_1,...,\mu_K)=\boldsymbol \mu  \tag{5.4}
     $$
     需要注意的是，$\bold y$ 用的是$one-hot$ 编码方式。

     所以，我们建立的线性模型，对每一个分量而言就是：
     $$
     h_{\bold w}^k(\bold x)=\mu_k=\frac {\exp(\bold w_k^T\bold x)}{\sum_{l=1}^K \exp(\bold w_l^T\bold x)} \tag{5.5}
     $$
     $h_{\bold w}^k(\bold x)$意思就是$\bold y$取第$k$类的概率，$(5.5)$就是$softmax \ \  regression$。

     这里说明一下$softmax \ \  function$:
     $$
     softmax(a_k)=\frac {\exp (a_k)}{\sum_j^K\exp(a_j)} \tag{5.6}
     $$
     它可以由贝叶斯定理推到出来：
     $$
     p(C_k|\bold x)=\frac {p(\bold x|C_k)p(C_k)}{\sum_j^K p(\bold x|C_j)p(C_j)}=\frac {\exp (a_k)}{\sum_j^K\exp(a_j)}=softmax(a_k) \tag{5.7}
     $$
     其中：$a_k=\ln (p(\bold x|C_k)p(C_k))$

  3. 高斯分布：用于回归问题

     当目标变量$y$是连续的，也即是面对的是回归问题的时候，如果，我们选择的概率模型是高斯分布，即$p(y|\bold x) \sim \mathcal N(y|\mu,\sigma^2)$，
     $$
     \begin{aligned}
     p(y|\mu,\sigma^2)
     &=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp(\frac{1}{2\sigma^2}(y-\mu)^2)\\
     &=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp(\frac{\mu}{\sigma^2}y-\frac{1}{2\sigma^2}y^2-\frac{1}{2\sigma^2}\mu^2)\\
     &=\frac{1}{(2\pi\sigma^2)^{1/2}}\exp(-\frac{1}{2\sigma^2}y^2)\exp(\frac{\mu}{\sigma^2}y-\frac{1}{2\sigma^2}\mu^2)
     \end{aligned}   \tag{5.6}
     $$
     可以看出：
     $$
     \eta =\frac{\mu}{\sigma^2}   \tag{5.7}
     $$
     所建线性模型：
     $$
     h_{\bold w}(\bold x)=E( y|\bold x;\bold w)=\mu=\eta\sigma^2=\sigma^2\bold w^T\bold x=(\bold w')^T\bold x \tag {5.8}
     $$
     从(5.8)式，$\bold w'$只是$\bold w$的缩放，所以我们在建模的时候，直接写成$h_{\bold w}(\bold x)=\bold w^T\bold x$，不会影响结果。

+ 线性模型建立好后，如何估计出参数？

  对于$logistic$和$softmax$，因为其建立的线性模型仍是表示概率，所以可以用最大似然估计来估计参数。

  对于回归问题，就要给出损失函数，其实对于分类问题，也是给出损失函数的，只是分类问题中的最大似然和交叉熵是等价的，即用最大似然，就是对交叉熵进行优化。会说回来，回归问题的损失函数可以采用均方损失：
  $$
  L=\frac {1}{2}\sum_{n=1}^N(y-h_{\bold w}(\bold x))^2=\frac {1}{2}\sum_{n=1}^N(y-\bold w^T\bold x)^2   \tag6
  $$
  这和选择：
  $$
  y=\bold w^T \bold x+\epsilon\\
  \epsilon \sim \mathcal N(\epsilon|0,\sigma^2)\\
  p(y|\bold x,\bold w)=\mathcal N(y|\bold w^T \bold x,\sigma^2)
  $$
  负对数似然：
  $$
  -LL=\frac {1}{2\sigma^2}\sum_{n=1}^N(y-\bold w^T\bold x)^2+const  \tag{6.1}
  $$
  因为要优化的是$\bold w$，与$\sigma^2$无关，$\sigma^2$可以看出一个常值，故上式就变成了式$(6)$。

​     